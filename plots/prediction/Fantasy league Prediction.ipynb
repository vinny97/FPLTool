{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Vinny/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: matplotlib.pyplot as already been imported, this call will have no effect.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import gc\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from matplotlib import use\n",
    "use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import csv,argparse,os,shutil\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import csv,argparse,os,shutil\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline(df, is_train = False, create_interactions = False, fillna_dict = None,\\\n",
    "                 single_interaction_vars = None, higher_interaction_vars = None, df_grouping_train = None,\\\n",
    "                 target = None):\n",
    "    assert target != None\n",
    "    columns = [x.lower() for x in df.columns.tolist()]\n",
    "    df.columns = columns\n",
    "    df_etl = df.loc[:,:]\n",
    "    if is_train == True:\n",
    "        # apply some filter here\n",
    "        # df_etl = df_etl.loc[(~df_etl['dob'].isnull()) & (~df_etl['loan_period'].isnull())]\n",
    "        # df_etl = df_etl.loc[(~df_etl['dob'].isnull())]\n",
    "        pass\n",
    "        \n",
    "    vars_to_one_hot_encode = []\n",
    "\n",
    "    # encode basic variables\n",
    "    for col in vars_to_one_hot_encode:\n",
    "        for val in df_etl[col].unique():\n",
    "            # vars_for_model.append(col + '_' + str(val))\n",
    "            # print (vars_for_model[-1])\n",
    "            df_etl[col + '_' + str(val)] = df_etl[col].apply(lambda x: 1 if x == val else 0)\n",
    "    \n",
    "    # fill missing values\n",
    "    if fillna_dict is not None:\n",
    "        df_etl.fillna(fillna_dict, inplace = True)\n",
    "    \n",
    "    df_grouping = {}\n",
    "    if create_interactions == True and is_train == True:\n",
    "        if single_interaction_vars is not None:\n",
    "            for var_to_group in single_interaction_vars:\n",
    "                print (var_to_group)\n",
    "                a=df_etl.groupby([var_to_group]).agg({'total_points_target':[np.mean]})\n",
    " #               a.columns=list(map(lambda x: x[:-1] if x[-1] == '_' else x, ['_'.join(x) for x in a.columns.ravel()]))\n",
    " #               a['avg_ctr']=a['unique_clicks_sum']/a['unique_impression_sum']\n",
    "                df_grouping[var_to_group]=a\n",
    "                new_col_names =  [df_grouping[var_to_group].index.name] + ['_'.join([df_grouping[var_to_group].index.name] + list(x)) \\\n",
    "                                  for x in (df_grouping[var_to_group].columns.ravel())]\n",
    "                # print (new_col_names)\n",
    "                df_grouping[var_to_group].reset_index(inplace = True)\n",
    "                df_grouping[var_to_group].columns = new_col_names\n",
    "                for col in df_grouping[var_to_group].columns:\n",
    "                    if '_count' in col:\n",
    "                        df_grouping[var_to_group][col] = df_grouping[var_to_group][col]/len(df)\n",
    "\n",
    "                df_etl = pd.merge(left = df_etl, right = df_grouping[var_to_group], on = var_to_group, how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if higher_interaction_vars is not None:\n",
    "            for var_to_group in higher_interaction_vars:\n",
    "                print (var_to_group)\n",
    "                a=df_etl.groupby(list(var_to_group)).agg({'total_points_target':[np.mean]})\n",
    "           #     a.columns=list(map(lambda x: x[:-1] if x[-1] == '_' else x, ['_'.join(x) for x in a.columns.ravel()]))\n",
    "        #        a['avg_ctr']=a['unique_clicks_sum']/a['unique_impression_sum']\n",
    "                df_grouping[var_to_group]=a\n",
    "                new_col_names =  list(var_to_group) + ['_'.join(['_'.join(var_to_group)] + list(x)) \\\n",
    "                                  for x in (df_grouping[var_to_group].columns.ravel())]\n",
    "                df_grouping[var_to_group].reset_index(inplace = True)\n",
    "                df_grouping[var_to_group].columns = new_col_names\n",
    "                for col in df_grouping[var_to_group].columns:\n",
    "                    if '_count' in col[-6:]:\n",
    "                        df_grouping[var_to_group][col] = df_grouping[var_to_group][col]/len(df)\n",
    "\n",
    "                df_etl = pd.merge(left = df_etl, right = df_grouping[var_to_group], on = list(var_to_group), how = 'left')\n",
    "    \n",
    "    if is_train == False and create_interactions == True:\n",
    "        assert df_grouping_train is not None\n",
    "        for var_to_group in df_grouping_train.keys():\n",
    "            df_etl = pd.merge(left = df_etl, right = df_grouping_train[var_to_group], on = var_to_group, how = 'left')  \n",
    "    if is_train == True:\n",
    "            return (df_etl, df_grouping)\n",
    "    return (df_etl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_graph(inputdf, target_rate,population,variable,title):\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.bar(range(0,len(inputdf)), inputdf[population],color='c',label='#Customer',width=0.3)\n",
    "    ax1.set_ylabel('Population')\n",
    "    ax1.set_xlabel(variable)\n",
    "    plt.title(title)\n",
    "    plt.xticks(range(0,len(inputdf)), inputdf.index, rotation=45)\n",
    "    \n",
    "    ax1.text(1.1,0.30,'Lower',transform=ax1.transAxes)\n",
    "    ax1.text(1.1,0.24,'Targets',transform=ax1.transAxes)\n",
    "    ax1.annotate('', xy=(1.15,0.05), xycoords='axes fraction', xytext=(1.15,0.22),arrowprops=dict(arrowstyle=\"simple\", color='r'))\n",
    "    \n",
    "    ax1.text(1.1,0.70,'Higher',transform=ax1.transAxes)\n",
    "    ax1.text(1.1,0.65,'Targets',transform=ax1.transAxes)\n",
    "    ax1.annotate('', xy=(1.15,0.94), xycoords='axes fraction', xytext=(1.15,0.75),arrowprops=dict(arrowstyle=\"simple\", color='g'))\n",
    "    \n",
    "    ax2 = ax1.twinx()   \n",
    "    ax2.plot(range(0,len(inputdf)), inputdf[target_rate],color='g',label='Avg Points')\n",
    "    ax2.set_ylabel('Avg Points')\n",
    "    fig.savefig('Plots/%s.png'%variable)\n",
    "\n",
    "def get_next_range(arr,group_range,start):\n",
    "    if group_range + start >=100:\n",
    "        return 100\n",
    "    elif (100 - group_range/2) < start + group_range:\n",
    "        return 100\n",
    "    elif arr[-1] == arr[start]:\n",
    "        return 100\n",
    "    elif (arr[start+group_range] == arr[start]) or (arr[start] < 0):\n",
    "        return np.max([np.min(np.where(arr > arr[start])),np.min(np.where(arr >= 0))])\n",
    "    else:\n",
    "        return group_range + start\n",
    "    \n",
    "def get_risk_table_categorical(inputdf, variable , target, variable_desc, cutoff = 1000):\n",
    "    df1 = inputdf[[variable,target]]\n",
    "    df1[variable][df1[variable].isnull()]=\"MISSING\"\n",
    "    \n",
    "    freq1 = df1[variable].value_counts()\n",
    "    dict1 = {}\n",
    "    for key,value in zip(list(freq1.index), list(freq1)):\n",
    "        if value < cutoff:\n",
    "            dict1[key] = \"OTHER\"\n",
    "        else:\n",
    "            dict1[key] = str(key)\n",
    " \n",
    "    df1[\"Clean_%s\"%variable] = df1[[variable]].applymap(dict1.get)\n",
    "    df2 = df1.groupby([\"Clean_%s\"%variable]).agg([np.mean, np.sum, np.size ])\n",
    "    df2 = df2[target][['mean','sum','size']]\n",
    "\n",
    "    #df2 = pd.crosstab(df1[\"Clean_%s\"%variable], df1[target],colnames=[target])\n",
    "    #df2.columns = ['Bads','Goods']\n",
    "    df2['Avg Points'] = df2['mean']\n",
    "    df2['Population Percentage'] = 1.0*df2['size']/len(inputdf)\n",
    "    df2.sort_values('Avg Points', inplace=True)\n",
    "    df2['Variable'] = variable\n",
    "    plot_final_graph(df2,'Avg Points','Population Percentage',variable,variable_desc)\n",
    "    return df2\n",
    "\n",
    "def get_risk_table_numeric(df,var,target,groups,title,special_values):\n",
    "    df1 = df[[var,target]]\n",
    "    df2 = df[[var,target]]\n",
    "    if len(special_values) > 0:\n",
    "        df1.replace(special_values,[np.nan for x in special_values],inplace=True)\n",
    "    df3 = df2[df1[var].isnull()]\n",
    "    df3.fillna(-999,inplace=True)\n",
    "    df1.dropna(inplace=True)\n",
    "    \n",
    "    bins = []\n",
    "    begin_traverse = 0\n",
    "    percentiles = np.array([np.percentile(df1[var],p) for p in range(0,100)])\n",
    "    group_range = int(100/groups)\n",
    "    \n",
    "    while (begin_traverse <100):\n",
    "        bins += [percentiles[begin_traverse]]\n",
    "        begin_traverse = get_next_range(percentiles,group_range,begin_traverse)\n",
    "    \n",
    "    bins.append(np.max(df1[var])+1)\n",
    "    df1.loc[:,'BINS'] = pd.cut(df1[var], bins, right=False, labels=None, retbins=False, precision=3, include_lowest=False)\n",
    "    df4 = df1.groupby('BINS').agg([np.mean, np.sum, np.size ])\n",
    "    df5 = df4[target][['mean','sum','size']]\n",
    "    df5['Bin Mean'] = df4[var][['mean']]\n",
    "    df5['Avg Points'] = df5['mean']\n",
    "    df5['Population Percentage'] = 1.0*df5['size']/len(df)\n",
    "    df5.sort_values(by='Bin Mean', inplace=True) \n",
    "    \n",
    "    if len(df3) > 0:\n",
    "        df4 = df3.groupby(var).agg([ np.mean, sum, np.size ])\n",
    "        df6 = df4[target][['mean','sum','size']]\n",
    "        df6['Bin Mean'] = df4.index\n",
    "        df6['Avg Points'] = df6['mean']\n",
    "        df6['Population Percentage'] = 1.0*df6['size']/len(df)\n",
    "        df6.sort('Bin Mean', inplace=True) \n",
    "        df5 = pd.concat([df6,df5])\n",
    "    plot_final_graph(df5,'Avg Points','Population Percentage',var,title)\n",
    "    df5['Variable'] = var\n",
    "    return df5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_desc = pd.read_csv('player-info.csv')\n",
    "\n",
    "player_desc = player_desc[['element_type','id','team']]\n",
    "\n",
    "gw_data=pd.DataFrame()\n",
    "for i in range(12):\n",
    "    test = pd.read_csv('gws18/gw%s.csv'%(i+1), encoding = \"ISO-8859-1\")\n",
    "    gw_data = gw_data.append(test)\n",
    "\n",
    "gw_data = pd.merge(gw_data,player_desc,left_on='element',right_on='id',how='left')\n",
    "\n",
    "#gw_data_gk = gw_data[gw_data['element_type']==1]\n",
    "\n",
    "#gw_data_gk.describe().to_csv('gk_dd.csv')\n",
    "\n",
    "def func(x,var,win):\n",
    "    x[var+'_rolling_%s'%(win)] = x.rolling(on='round',window=win)[var].sum().fillna(0)\n",
    "    return x\n",
    "\n",
    "gw_data_gk = gw_data[['name',\n",
    "                         'assists',\n",
    "'attempted_passes',\n",
    "'big_chances_created',\n",
    "'bonus',\n",
    "'bps',\n",
    "'clean_sheets',\n",
    "'clearances_blocks_interceptions',\n",
    "'completed_passes',\n",
    "'creativity',\n",
    "'dribbles',\n",
    "'element',\n",
    "'errors_leading_to_goal',\n",
    "'errors_leading_to_goal_attempt',\n",
    "'fouls',\n",
    "'goals_conceded',\n",
    "'ict_index',\n",
    "'influence',\n",
    "'key_passes',\n",
    "'minutes',\n",
    "'opponent_team',\n",
    "'own_goals',\n",
    "'penalties_conceded',\n",
    "'penalties_saved',\n",
    "'recoveries',\n",
    "'round',\n",
    "'saves',\n",
    "'selected',\n",
    "'tackled',\n",
    "'tackles',\n",
    "'target_missed',\n",
    "'team_a_score',\n",
    "'team_h_score',\n",
    "'threat',\n",
    "'was_home',\n",
    "'total_points',\n",
    "'transfers_in',\n",
    "'transfers_out',\n",
    "'winning_goals',\n",
    "'yellow_cards',\n",
    "'team',\n",
    "'big_chances_missed',\n",
    "'ea_index',\n",
    "'goals_scored',\n",
    "'loaned_in',\n",
    "'loaned_out',\n",
    "'offside',\n",
    "'open_play_crosses',\n",
    "'penalties_missed',\n",
    "'red_cards',\n",
    "'element_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['assists',\n",
    "'attempted_passes',\n",
    "'big_chances_created',\n",
    "'bonus',\n",
    "'bps',\n",
    "'clean_sheets',\n",
    "'clearances_blocks_interceptions',\n",
    "'completed_passes',\n",
    "'creativity',\n",
    "'dribbles',\n",
    "'errors_leading_to_goal',\n",
    "'errors_leading_to_goal_attempt',\n",
    "'fouls',\n",
    "'goals_conceded',\n",
    "'ict_index',\n",
    "'influence',\n",
    "'key_passes',\n",
    "'minutes',\n",
    "'own_goals',\n",
    "'penalties_conceded',\n",
    "'penalties_saved',\n",
    "'recoveries',\n",
    "'saves',\n",
    "'selected',\n",
    "'tackled',\n",
    "'tackles',\n",
    "'target_missed',\n",
    "'threat',\n",
    "'total_points',\n",
    "'transfers_in',\n",
    "'transfers_out',\n",
    "'winning_goals',\n",
    "'yellow_cards',\n",
    "'big_chances_missed',\n",
    "'ea_index',\n",
    "'goals_scored',\n",
    "'loaned_in',\n",
    "'loaned_out',\n",
    "'offside',\n",
    "'open_play_crosses',\n",
    "'penalties_missed',\n",
    "'red_cards']:\n",
    "    gw_data_gk = gw_data_gk.groupby('element').apply(func,col,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gw_data_gk[gw_data_gk['round']>=1]\n",
    "data_target = data.copy()\n",
    "data_target['round'] = data_target['round']-1\n",
    "data_target = data_target[['round','element','total_points','opponent_team','was_home']]\n",
    "data_target = data_target.rename(index=str, columns={\"total_points\": \"total_points_target\"})\n",
    "data = pd.merge(data,data_target,on=['round','element'],how='left')\n",
    "data = data[~data['element_type'].isnull()]\n",
    "data = data[(data['minutes']>0) &(~data['was_home_y'].isnull())]\n",
    "data['was_home_y'] = data['was_home_y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['total_points_target'].isnull())]\n",
    "test = data[data['round']>=1]\n",
    "train = data[~(data['round']>=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "team\n",
      "opponent_team_y\n",
      "was_home_y\n",
      "element_type\n",
      "('team', 'was_home_y')\n",
      "('team', 'element_type')\n",
      "('was_home_y', 'element_type')\n",
      "('opponent_team_y', 'element_type')\n",
      "('opponent_team_y', 'was_home_y')\n"
     ]
    }
   ],
   "source": [
    "df_grouping_train=etl_pipeline(df=train,is_train = True, create_interactions = True, fillna_dict = None,\n",
    "                 single_interaction_vars = ['team','opponent_team_y','was_home_y','element_type'], \n",
    "             higher_interaction_vars = [('team','was_home_y'),\n",
    "                                        ('team','element_type'),\n",
    "                                        ('was_home_y','element_type'),\n",
    "                                        ('opponent_team_y','element_type'),\n",
    "                                        ('opponent_team_y','was_home_y')], df_grouping_train = None,\n",
    "                 target = 'total_points_target')\n",
    "\n",
    "\n",
    "df_grouping_test=etl_pipeline(df=test,is_train = False, create_interactions = True, fillna_dict = None,\n",
    "               single_interaction_vars = ['team','opponent_team_y','was_home_y','element_type'], \n",
    "              higher_interaction_vars = [('team','was_home_y'),\n",
    "                                         ('team','element_type'),\n",
    "                                         ('was_home_y','element_type'),\n",
    "                                         ('opponent_team_y','element_type'),\n",
    "                                         ('opponent_team_y','was_home_y')], df_grouping_train = df_grouping_train[1],\n",
    "                 target = 'total_points_target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean=df_grouping_train[0]\n",
    "test_clean=df_grouping_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_clean_gk = test_clean[test_clean['element_type'].isin([3,4])]\n",
    "#train_clean_gk = train_clean[train_clean['element_type'].isin([3,4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = (predictions - test_labels)\n",
    "    me = 100 * np.mean(errors) / np.mean(test_labels)\n",
    "    mae = 100 * np.mean(abs(errors)) / np.mean(test_labels)\n",
    "    \n",
    "    print('Mean Error By Mean: {:0.2f}%.'.format(me))\n",
    "    print('Mean Abs Error By Mean = {:0.2f}%.'.format(mae))\n",
    "    print('Root Mean Sq Error = {:0.2f}%.'.format(sqrt(mean_squared_error(test_labels, predictions))))\n",
    "\n",
    "def train_rf_cv(train_clean,test_clean,var_remove):    \n",
    "    param_grid = {\n",
    "        'max_depth': [ 2,3,4,5,6],\n",
    "        'min_samples_leaf': [5,10,20, 30],\n",
    "        'n_estimators': [ 1,2,3,4,5,10]}\n",
    "    rf = RandomForestRegressor()\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "    grid_search.fit(train_clean[idv].drop(var_remove,axis=1), train_clean['total_points_target'])\n",
    "\n",
    "    grid_search.best_params_\n",
    "\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    print (\"Train Accuracy... \\n\")\n",
    "    grid_accuracy = evaluate(best_grid, train_clean[idv].drop(var_remove,axis=1), train_clean['total_points_target'])\n",
    "    print (\"\\n Test Accuracy\")\n",
    "    grid_accuracy = evaluate(best_grid, test_clean[idv].drop(var_remove,axis=1), test_clean['total_points_target'])\n",
    "    \n",
    "    feature_import = pd.DataFrame(data=best_grid.feature_importances_, index=train_clean[idv].drop(var_remove,axis=1).columns.values, columns=['values'])\n",
    "    feature_import.sort_values(['values'], ascending=False, inplace=True)\n",
    "    feature_import.reset_index(level=0, inplace=True)\n",
    "    sns.barplot(x='index', y='values', data=feature_import, palette='deep')\n",
    "    plt.show()\n",
    "    return (best_grid,feature_import.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "idv=[\n",
    "    'name',\n",
    "    'total_points_target',\n",
    "    'total_points',\n",
    "    'team_total_points_target_mean',\n",
    "    'opponent_team_y_total_points_target_mean',\n",
    "    'was_home_y_total_points_target_mean',\n",
    "    'team_was_home_y_total_points_target_mean',\n",
    "    'opponent_team_y_was_home_y_total_points_target_mean',\n",
    "    'team_element_type_total_points_target_mean',\n",
    "    'was_home_y_element_type_total_points_target_mean',\n",
    "    'opponent_team_y_element_type_total_points_target_mean',\n",
    "    'element_type_total_points_target_mean',\n",
    "    'assists_rolling_10',\n",
    "'attempted_passes_rolling_10',\n",
    "'big_chances_created_rolling_10',\n",
    "'bonus_rolling_10',\n",
    "'bps_rolling_10',\n",
    "'clean_sheets_rolling_10',\n",
    "'clearances_blocks_interceptions_rolling_10',\n",
    "'completed_passes_rolling_10',\n",
    "'creativity_rolling_10',\n",
    "'dribbles_rolling_10',\n",
    "'errors_leading_to_goal_rolling_10',\n",
    "'errors_leading_to_goal_attempt_rolling_10',\n",
    "'fouls_rolling_10',\n",
    "'goals_conceded_rolling_10',\n",
    "'ict_index_rolling_10',\n",
    "'influence_rolling_10',\n",
    "'key_passes_rolling_10',\n",
    "'minutes_rolling_10',\n",
    "'own_goals_rolling_10',\n",
    "'penalties_conceded_rolling_10',\n",
    "'penalties_saved_rolling_10',\n",
    "'recoveries_rolling_10',\n",
    "'saves_rolling_10',\n",
    "'selected_rolling_10',\n",
    "'tackled_rolling_10',\n",
    "'tackles_rolling_10',\n",
    "'target_missed_rolling_10',\n",
    "'threat_rolling_10',\n",
    "'total_points_rolling_10',\n",
    "'transfers_in_rolling_10',\n",
    "'transfers_out_rolling_10',\n",
    "'winning_goals_rolling_10',\n",
    "'yellow_cards_rolling_10',\n",
    "'big_chances_missed_rolling_10',\n",
    "'goals_scored_rolling_10',\n",
    "'offside_rolling_10',\n",
    "'open_play_crosses_rolling_10',\n",
    "'penalties_missed_rolling_10',\n",
    "'red_cards_rolling_10'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c53b60d703c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_with_ctr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_rf_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_clean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_points_target'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-d8eba772111c>\u001b[0m in \u001b[0;36mtrain_rf_cv\u001b[0;34m(train_clean, test_clean, var_remove)\u001b[0m\n\u001b[1;32m     17\u001b[0m     grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n\u001b[1;32m     18\u001b[0m                           cv = 3, n_jobs = -1, verbose = 2)\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_remove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_points_target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 (\"Cannot have number of splits n_splits={0} greater\"\n\u001b[1;32m    328\u001b[0m                  \" than the number of samples: n_samples={1}.\")\n\u001b[0;32m--> 329\u001b[0;31m                 .format(self.n_splits, n_samples))\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot have number of splits n_splits=3 greater than the number of samples: n_samples=0."
     ]
    }
   ],
   "source": [
    "model_with_ctr=train_rf_cv(train_clean,test_clean,['total_points_target','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_cv(train_clean,test_clean,var_remove):    \n",
    "    param_grid = {\n",
    "        'alpha': [ 0.1,0.3,0.5,1,2],\n",
    "        'l1_ratio': [0,0.5, 1],\n",
    "        'normalize': [ True, False]}\n",
    "    rf = ElasticNet(max_iter=5000)\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2, scoring ='neg_mean_squared_error')\n",
    "    grid_search.fit(train_clean[idv].drop(var_remove,axis=1), train_clean['total_points_target'])\n",
    "\n",
    "    grid_search.best_params_\n",
    "\n",
    "\n",
    "    best_grid = grid_search.best_estimator_\n",
    "    print (\"Train Accuracy... \\n\")\n",
    "    grid_accuracy = evaluate(best_grid, train_clean[idv].drop(var_remove,axis=1), train_clean['total_points_target'])\n",
    "    print (\"\\n Test Accuracy\")\n",
    "    grid_accuracy = evaluate(best_grid, test_clean[idv].drop(var_remove,axis=1), test_clean['total_points_target'])\n",
    "    \n",
    "    feature_import = pd.DataFrame(data=best_grid.coef_, index=train_clean[idv].drop(var_remove,axis=1).columns.values, columns=['values'])\n",
    "    feature_import.sort_values(['values'], ascending=False, inplace=True)\n",
    "    feature_import.reset_index(level=0, inplace=True)\n",
    "    sns.barplot(x='index', y='values', data=feature_import, palette='deep')\n",
    "    plt.show()\n",
    "    return (best_grid,feature_import.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_ctr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_with_ctr_lr=train_lr_cv(train_clean,test_clean,['total_points_target','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_risk_table_numeric(train_clean_gk,'clean_sheets_rolling_10','total_points_target',10,'clean_sheets',special_values=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_ctr[1].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_11 = test_clean.copy()\n",
    "test_features = test_clean[idv].drop(['total_points_target','name'],axis=1)\n",
    "test_labels = test_clean['total_points_target']\n",
    "predictions = model_with_ctr[0].predict(test_features)\n",
    "test_clean['prediction'] = predictions\n",
    "test_clean['round'] = test_clean['round'] + 1\n",
    "\n",
    "test_clean[['name','element_type','round','prediction']].to_csv('PlayerPredictionAfterGW28.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Best 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_clean_11[idv].drop(['total_points_target','name'],axis=1)\n",
    "test_clean_11['prediction'] = model_with_ctr[0].predict(test_features)\n",
    "test_clean_11 = test_clean_11.drop_duplicates(['name','round'],keep='first')\n",
    "\n",
    "test_clean_11=test_clean_11.groupby(['name','element_type']).agg({'prediction':'sum','total_points_target':'sum'}).reset_index()\n",
    "\n",
    "test_labels = test_clean_11['total_points_target']\n",
    "predictions = test_clean_11['prediction']\n",
    "#print(\"mean squared error \"+str(sqrt(mean_squared_error(test_labels, predictions))))\n",
    "test_clean['prediction'] = predictions\n",
    "\n",
    "\n",
    "#test_clean_gk[['name','team','round','total_points_rolling_10','opponent_team_y','was_home_y','prediction','total_points_target']].to_csv('predGk.csv')\n",
    "#test_clean = test_clean.drop_duplicates(['name','round'],keep='first')\n",
    "\n",
    "gk_ordered_pred=pd.DataFrame()\n",
    "for x in test_clean_11.groupby(['element_type']):\n",
    "    df_temp = x[1] \n",
    "    df_temp = df_temp.sort_values(by='prediction',ascending =False)\n",
    "    df_temp['player_rank_pred']=list(range(1,len(df_temp)+1))\n",
    "  #  df_temp['round'] = x[0][0]\n",
    "    df_temp['element_type'] = x[0]\n",
    "    gk_ordered_pred = gk_ordered_pred.append(df_temp.copy())\n",
    "\n",
    "gk_ordered_actual=pd.DataFrame()\n",
    "   \n",
    "for x in test_clean_11.groupby(['element_type']):\n",
    "    df_temp = x[1] \n",
    "    df_temp = df_temp.sort_values(by='total_points_target',ascending =False)\n",
    "    df_temp['player_rank_actual']=list(range(1,len(df_temp)+1))\n",
    "   # df_temp['round'] = x[0][0]\n",
    "    df_temp['element_type'] = x[0]\n",
    "    gk_ordered_actual = gk_ordered_actual.append(df_temp.copy())\n",
    "    \n",
    "\n",
    "# ordering = pd.merge(gk_ordered_actual[['name','round','team','opponent_team_y','was_home_y','player_rank_actual','total_points_target','prediction']],\n",
    "#                     gk_ordered_pred[['name','round','player_rank_pred','element_type']],how='left',on=['name','round'])\n",
    "ordering = pd.merge(gk_ordered_actual,\n",
    "                     gk_ordered_pred,how='left',on=['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation = [1,4,3,3]\n",
    "best_eleven = pd.DataFrame()\n",
    "for i,v in enumerate(formation):\n",
    "    best_eleven = best_eleven.append(ordering[(ordering['element_type_x']==i+1) & (ordering['player_rank_pred']<=v)])\n",
    "    \n",
    "best_eleven[['name','element_type_x']].to_csv('Best_Eleven.csv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean_copy = test_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = test_clean_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_features = test_clean[idv].drop(['total_points_target','name'],axis=1)\n",
    "test_clean['predictions'] = model_with_ctr[0].predict(test_features)\n",
    "test_clean = test_clean.drop_duplicates(['name','round'],keep='first')\n",
    "\n",
    "test_clean=test_clean.groupby(['name','element_type']).agg({'prediction':'sum','total_points_target':'sum'}).reset_index()\n",
    "\n",
    "test_labels = test_clean['total_points_target']\n",
    "predictions = test_clean['prediction']\n",
    "print(\"mean squared error \"+str(sqrt(mean_squared_error(test_labels, predictions))))\n",
    "test_clean['prediction'] = predictions\n",
    "\n",
    "\n",
    "#test_clean_gk[['name','team','round','total_points_rolling_10','opponent_team_y','was_home_y','prediction','total_points_target']].to_csv('predGk.csv')\n",
    "#test_clean = test_clean.drop_duplicates(['name','round'],keep='first')\n",
    "\n",
    "gk_ordered_pred=pd.DataFrame()\n",
    "for x in test_clean.groupby(['element_type']):\n",
    "    df_temp = x[1] \n",
    "    df_temp = df_temp.sort_values(by='prediction',ascending =False)\n",
    "    df_temp['player_rank_pred']=list(range(1,len(df_temp)+1))\n",
    "  #  df_temp['round'] = x[0][0]\n",
    "    df_temp['element_type'] = x[0]\n",
    "    gk_ordered_pred = gk_ordered_pred.append(df_temp.copy())\n",
    "\n",
    "gk_ordered_actual=pd.DataFrame()\n",
    "   \n",
    "for x in test_clean.groupby(['element_type']):\n",
    "    df_temp = x[1] \n",
    "    df_temp = df_temp.sort_values(by='total_points_target',ascending =False)\n",
    "    df_temp['player_rank_actual']=list(range(1,len(df_temp)+1))\n",
    "   # df_temp['round'] = x[0][0]\n",
    "    df_temp['element_type'] = x[0]\n",
    "    gk_ordered_actual = gk_ordered_actual.append(df_temp.copy())\n",
    "    \n",
    "\n",
    "# ordering = pd.merge(gk_ordered_actual[['name','round','team','opponent_team_y','was_home_y','player_rank_actual','total_points_target','prediction']],\n",
    "#                     gk_ordered_pred[['name','round','player_rank_pred','element_type']],how='left',on=['name','round'])\n",
    "ordering = pd.merge(gk_ordered_actual,\n",
    "                     gk_ordered_pred,how='left',on=['name'])\n",
    "\n",
    "n=11\n",
    "a = ordering [ordering['player_rank_actual']<=n]\n",
    "a['correct'] = np.where(a['player_rank_pred']<=n,1,0)\n",
    "print(\"Out of Top \"+str(n)+\", \"+str(a['correct'].mean())+\" were correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordering.to_csv('sangam_tset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_total_score(x):\n",
    "    \n",
    "    pred = x[(x['element_type']==1) & (x['player_rank_pred']<=1)]['total_points_target'].sum()\n",
    "    best = x[(x['element_type']==1) & (x['player_rank_actual']<=1)]['total_points_target'].sum()\n",
    "    \n",
    "    pred_def = x[(x['element_type']==2) & (x['player_rank_pred']<=4)]['total_points_target'].sum()\n",
    "    best_def = x[(x['element_type']==2) & (x['player_rank_actual']<=4)]['total_points_target'].sum()\n",
    "    \n",
    "    pred_mid = x[(x['element_type']==3) & (x['player_rank_pred']<=3)]['total_points_target'].sum()\n",
    "    best_mid = x[(x['element_type']==3) & (x['player_rank_actual']<=3)]['total_points_target'].sum()\n",
    "   \n",
    "    pred_fw = x[(x['element_type']==4) & (x['player_rank_pred']<=3)]['total_points_target'].sum()\n",
    "    best_fw = x[(x['element_type']==4) & (x['player_rank_actual']<=3)]['total_points_target'].sum()\n",
    "    d = {}\n",
    "    d['pred_gk'] = pred\n",
    "    d['best_gk'] = best\n",
    "    d['pred_def'] = pred_def\n",
    "    d['best_def'] = best_def    \n",
    "    d['pred_mid'] = pred_mid\n",
    "    d['best_mid'] = best_mid\n",
    "    d['pred_fw'] = pred_fw\n",
    "    d['best_fw'] = best_fw  \n",
    "    d['pred_total'] = pred+pred_def+pred_mid+pred_fw\n",
    "    d['best_total'] = best+best_def+best_mid+best_fw\n",
    "    return pd.Series(d,index=['pred_gk','best_gk','pred_def','best_def','pred_mid','best_mid','pred_fw','best_fw','pred_total','best_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ordering[(ordering['round']==16)]\n",
    "x[(x['element_type']==4) & (x['player_rank_actual']<=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ordering[(ordering['round']==16)]\n",
    "x[(x['element_type']==4) & (x['player_rank_pred']<=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ordering.groupby('round').apply(performance_total_score).reset_index()\n",
    "print(a['pred_fw'].sum(),a['best_fw'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_risk_table_numeric(data,'assists_rolling_10','total_points_target',10,'assists',special_values=[])\n",
    "get_risk_table_numeric(data,'attempted_passes_rolling_10','total_points_target',10,'attempted_passes',special_values=[])\n",
    "get_risk_table_numeric(data,'big_chances_created_rolling_10','total_points_target',10,'big_chances_created',special_values=[])\n",
    "get_risk_table_numeric(data,'bonus_rolling_10','total_points_target',10,'bonus',special_values=[])\n",
    "get_risk_table_numeric(data,'bps_rolling_10','total_points_target',10,'bps',special_values=[])\n",
    "get_risk_table_numeric(data,'clean_sheets_rolling_10','total_points_target',10,'clean_sheets',special_values=[])\n",
    "get_risk_table_numeric(data,'clearances_blocks_interceptions_rolling_10','total_points_target',10,'clearances_blocks_interceptions',special_values=[])\n",
    "get_risk_table_numeric(data,'completed_passes_rolling_10','total_points_target',10,'completed_passes',special_values=[])\n",
    "get_risk_table_numeric(data,'creativity_rolling_10','total_points_target',10,'creativity',special_values=[])\n",
    "get_risk_table_numeric(data,'dribbles_rolling_10','total_points_target',10,'dribbles',special_values=[])\n",
    "get_risk_table_numeric(data,'errors_leading_to_goal_rolling_10','total_points_target',10,'errors_leading_to_goal',special_values=[])\n",
    "get_risk_table_numeric(data,'errors_leading_to_goal_attempt_rolling_10','total_points_target',10,'errors_leading_to_goal_attempt',special_values=[])\n",
    "get_risk_table_numeric(data,'fouls_rolling_10','total_points_target',10,'fouls',special_values=[])\n",
    "get_risk_table_numeric(data,'goals_conceded_rolling_10','total_points_target',10,'goals_conceded',special_values=[])\n",
    "get_risk_table_numeric(data,'ict_index_rolling_10','total_points_target',10,'ict_index',special_values=[])\n",
    "get_risk_table_numeric(data,'influence_rolling_10','total_points_target',10,'influence',special_values=[])\n",
    "get_risk_table_numeric(data,'key_passes_rolling_10','total_points_target',10,'key_passes',special_values=[])\n",
    "get_risk_table_numeric(data,'minutes_rolling_10','total_points_target',10,'minutes',special_values=[])\n",
    "get_risk_table_numeric(data,'own_goals_rolling_10','total_points_target',10,'own_goals',special_values=[])\n",
    "get_risk_table_numeric(data,'penalties_conceded_rolling_10','total_points_target',10,'penalties_conceded',special_values=[])\n",
    "get_risk_table_numeric(data,'penalties_saved_rolling_10','total_points_target',10,'penalties_saved',special_values=[])\n",
    "get_risk_table_numeric(data,'recoveries_rolling_10','total_points_target',10,'recoveries',special_values=[])\n",
    "get_risk_table_numeric(data,'saves_rolling_10','total_points_target',10,'saves',special_values=[])\n",
    "get_risk_table_numeric(data,'selected_rolling_10','total_points_target',10,'selected',special_values=[])\n",
    "get_risk_table_numeric(data,'tackled_rolling_10','total_points_target',10,'tackled',special_values=[])\n",
    "get_risk_table_numeric(data,'tackles_rolling_10','total_points_target',10,'tackles',special_values=[])\n",
    "get_risk_table_numeric(data,'target_missed_rolling_10','total_points_target',10,'target_missed',special_values=[])\n",
    "get_risk_table_numeric(data,'threat_rolling_10','total_points_target',10,'threat',special_values=[])\n",
    "get_risk_table_numeric(data,'total_points_rolling_10','total_points_target',10,'total_points',special_values=[])\n",
    "get_risk_table_numeric(data,'transfers_in_rolling_10','total_points_target',10,'transfers_in',special_values=[])\n",
    "get_risk_table_numeric(data,'transfers_out_rolling_10','total_points_target',10,'transfers_out',special_values=[])\n",
    "get_risk_table_numeric(data,'winning_goals_rolling_10','total_points_target',10,'winning_goals',special_values=[])\n",
    "get_risk_table_numeric(data,'yellow_cards_rolling_10','total_points_target',10,'yellow_cards',special_values=[])\n",
    "get_risk_table_numeric(data,'big_chances_missed_rolling_10','total_points_target',10,'big_chances_missed',special_values=[])\n",
    "get_risk_table_numeric(data,'ea_index_rolling_10','total_points_target',10,'ea_index',special_values=[])\n",
    "get_risk_table_numeric(data,'goals_scored_rolling_10','total_points_target',10,'goals_scored',special_values=[])\n",
    "get_risk_table_numeric(data,'loaned_in_rolling_10','total_points_target',10,'loaned_in',special_values=[])\n",
    "get_risk_table_numeric(data,'loaned_out_rolling_10','total_points_target',10,'loaned_out',special_values=[])\n",
    "get_risk_table_numeric(data,'offside_rolling_10','total_points_target',10,'offside',special_values=[])\n",
    "get_risk_table_numeric(data,'open_play_crosses_rolling_10','total_points_target',10,'open_play_crosses',special_values=[])\n",
    "get_risk_table_numeric(data,'penalties_missed_rolling_10','total_points_target',10,'penalties_missed',special_values=[])\n",
    "get_risk_table_numeric(data,'red_cards_rolling_10','total_points_target',10,'red_cards',special_values=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_risk_table_categorical(data, 'team' , 'total_points_target', 'Team', cutoff = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_risk_table_categorical(data, 'team' , 'total_points_target', 'Team', cutoff = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
